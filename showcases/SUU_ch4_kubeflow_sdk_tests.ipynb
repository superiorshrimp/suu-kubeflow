{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_A6dIR7muE2"
      },
      "outputs": [],
      "source": [
        "!pip install kubeflow-training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers datasets accelerate evaluate"
      ],
      "metadata": {
        "id": "dX2VOy09m_Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AdamW,\n",
        "    get_scheduler,\n",
        ")\n",
        "\n",
        "import transformers\n",
        "\n",
        "\n",
        "from datasets import (\n",
        "    Dataset,\n",
        "    load_dataset\n",
        ")\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import evaluate\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YnqjcUfqnBoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_func():\n",
        "    import torch\n",
        "    import os\n",
        "    import evaluate\n",
        "    import transformers\n",
        "    from transformers import (\n",
        "      AutoTokenizer,\n",
        "      AutoModelForSequenceClassification,\n",
        "      DataCollatorWithPadding,\n",
        "      Trainer,\n",
        "      TrainingArguments,\n",
        "      AdamW,\n",
        "      get_scheduler,\n",
        "    )\n",
        "\n",
        "    from datasets import (\n",
        "        Dataset,\n",
        "        load_dataset\n",
        "    )\n",
        "\n",
        "    def preprocess_dataset(examples):\n",
        "      input = [f\"{qt} {qc}? {ba}\" for qt, qc, ba in zip(examples[\"question_title\"], examples[\"question_content\"], examples[\"best_answer\"])]\n",
        "      tokenized = tokenizer(input, truncation=True, padding=False)\n",
        "      return {\"input_ids\": tokenized[\"input_ids\"], \"attention_mask\": tokenized[\"attention_mask\"], \"labels\": examples[\"topic\"]}\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    model_name = \"FacebookAI/roberta-large\"\n",
        "    dataset_name = \"yahoo_answers_topics\"\n",
        "\n",
        "    metric = evaluate.load(\"f1\")\n",
        "\n",
        "    # Download dataset.\n",
        "    dataset = load_dataset(dataset_name, split=\"train[:20%]\")\n",
        "    label_names = dataset.features['topic'].names\n",
        "    id2label = dict(enumerate(label_names))\n",
        "    label2id = {label: i for i, label in id2label.items()}\n",
        "\n",
        "    # Create model & tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
        "                                                              num_labels=len(label_names),\n",
        "                                                              id2label=id2label,\n",
        "                                                              label2id=label2id,)\n",
        "\n",
        "    #preprocess dataset\n",
        "    tokenized_dataset = dataset.map(preprocess_dataset, batched=True, remove_columns=[\"id\", \"topic\", \"question_title\", \"question_content\", \"best_answer\"])\n",
        "\n",
        "    train_validation = tokenized_dataset.train_test_split(test_size=0.1, seed=0)\n",
        "\n",
        "    ready_dataset = datasets.DatasetDict({\n",
        "        \"train\": train_validation['train'],\n",
        "        \"validation\": train_validation['test'],\n",
        "        \"test\": load_dataset(dataset_name, split=\"test[:5%]\").map(preprocess_dataset, batched=True, remove_columns=[\"id\", \"topic\", \"question_title\", \"question_content\", \"best_answer\"]),\n",
        "    })\n",
        "\n",
        "    # Attach model to PyTorch distributor.\n",
        "    torch.distributed.init_process_group(backend=\"nccl\")\n",
        "    Distributor = torch.nn.parallel.DistributedDataParallel\n",
        "    model = Distributor(model)\n",
        "\n",
        "    ###----1----###\n",
        "    #setting-up the trainer object - this part probably won't work -> we might need to move a level down - to pure torch training\n",
        "    trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir='models-roberta',\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=2,\n",
        "        weight_decay=0.01,\n",
        "        metric_for_best_model='f1',\n",
        "        load_best_model_at_end=True,\n",
        "        report_to='none',\n",
        "    ),\n",
        "    train_dataset=ready_dataset[\"train\"],\n",
        "    eval_dataset=ready_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    )\n",
        "    # Start model training.\n",
        "    trainer.train()\n",
        "    ###----1----###\n",
        "\n",
        "    ###----2----###\n",
        "    #alternate version of setting-up training - pure pytorch - has a higher chance of working than the one above\n",
        "    ready_datasets.set_format(\"torch\")\n",
        "    train_dataloader = DataLoader(ready_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator)\n",
        "    eval_dataloader = DataLoader(ready_datasets[\"validation\"], batch_size=8, collate_fn=data_collator)\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    num_epochs = 2\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    model.train()\n",
        "    #train the model with pytorch\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in train_dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    #eval model with pytorch\n",
        "    model.eval()\n",
        "    for batch in eval_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "    metric.compute()\n",
        "    ###----2----###"
      ],
      "metadata": {
        "id": "f1RO5SNAn_Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start PyTorchJob with 100 Workers and 2 GPUs per Worker.\n",
        "from kubeflow.training import TrainingClient\n",
        "TrainingClient().create_job(\n",
        "    name=\"pytorch-ddp\",\n",
        "    func=train_func,\n",
        "    num_workers=100,\n",
        "    resources_per_worker={\"gpu\": \"2\"},\n",
        ")"
      ],
      "metadata": {
        "id": "tOQVCVkFpgnF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}